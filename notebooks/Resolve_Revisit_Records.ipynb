{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading and resolving  *revisit* records in Web archive datasets\n",
    "\n",
    "Revisit records may be included in a dataset to avoid duplicates and save storage. In order to access the actual content such captures, the records need to be resolved first, i.e., the original record has to be identified in the CDX and additional meta information as well as the location information of the corresponding WARC records are *copied* to the revisit record.\n",
    "\n",
    "More information on *revisit records* can be found here:\n",
    "https://iipc.github.io/warc-specifications/specifications/warc-format/warc-1.1/#example-of-revisit-record"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import org.archive.archivespark._\n",
    "import org.archive.archivespark.implicits._\n",
    "import org.archive.archivespark.enrich.functions._\n",
    "import org.archive.archivespark.specific.warc._\n",
    "import org.archive.archivespark.specific.warc.enrichfunctions._\n",
    "import org.archive.archivespark.specific.warc.implicits._\n",
    "import org.archive.archivespark.specific.warc.specs._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the dataset from CDX + WARC files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "val path = \"/data/archiveit/ArchiveIt-Collection-2950\"\n",
    "val cdxPath = path + \"/cdx/*.cdx.gz\"\n",
    "val warcPath = path + \"/warc\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### As the CDX files contain revisit records, we only load these first (`CdxHdfsSpec`) to resolved them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "val unresolved = ArchiveSpark.load(CdxHdfsSpec(cdxPath))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Depending on the number of records in the dataset, increasing the parallelism for this operation may be a good idea to prevent out-of-memory errors as this is a relatively expensive operation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ArchiveSpark.parallelism = 10000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After resolving the revisit records, we can reduce the number of partitions again (`coalesce`), which has become large due to the high parallelism. Finally, the resolved CDX records are cached so that we do not need to compute them multiple times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "val resolved = unresolved.resolveRevisits().coalesce(1000).cache"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we continue with other operations, the parallelism should be decreased again as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ArchiveSpark.parallelism = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We can now load the dataset with the actual WARC files using the resolved CDX (`WarcHdfsCdxRddSpec`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "val records = ArchiveSpark.load(WarcHdfsCdxRddSpec(resolved, warcPath))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From here you can work with the dataset as usual, like shown in other [recipes](https://github.com/helgeho/ArchiveSpark/blob/master/docs/Recipes.md).\n",
    "\n",
    "For more details on the used [DataSpecs](https://github.com/helgeho/ArchiveSpark/blob/master/docs/DataSpecs.md) please [read the docs](https://github.com/helgeho/ArchiveSpark/blob/master/docs/README.md)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparing resolved and unresolved records"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We lose only four records for which no corresponding record was found to resolve it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "52922792"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unresolved.count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "52922788"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resolved.count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "52922788"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "records.count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "As expected, all revisit records are turned into their actual counterparts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17502250"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unresolved.filter(_.mime == \"warc/revisit\").count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resolved.filter(_.mime == \"warc/revisit\").count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "records.filter(_.mime == \"warc/revisit\").count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Storing and loading resolved CDX records\n",
    "\n",
    "If you often work with this dataset, you can save the resolved CDX records to avoid the resolving process next time (adding the .gz extension to the directory name automatically ensures that the CDX files will be compressed using GZip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "records.saveAsCdx(path + \"/resolved_cdx.gz\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ArchiveSpark latest (Spark 2.2.0) dynamic alloc",
   "language": "",
   "name": "archivespark_dynamic"
  },
  "language_info": {
   "name": "scala",
   "version": "2.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
