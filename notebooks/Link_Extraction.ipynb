{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extracting hyperlinks from webpages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import org.archive.archivespark._\n",
    "import org.archive.archivespark.implicits._\n",
    "import org.archive.archivespark.enrich.functions._\n",
    "import org.archive.archivespark.specific.warc._\n",
    "import org.archive.archivespark.specific.warc.enrichfunctions._\n",
    "import org.archive.archivespark.specific.warc.implicits._\n",
    "import org.archive.archivespark.specific.warc.specs._\n",
    "import org.apache.hadoop.io.compress.GzipCodec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the dataset\n",
    "\n",
    "In the example, the Web archive dataset will be loaded from local WARC / CDX files, using `WarcCdxHdfsSpec`, but any other [Data Specification (DataSpec)](https://github.com/helgeho/ArchiveSpark/blob/master/docs/DataSpecs.md) can be used here as well in order to load your records from different local or remote sources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "val path = \"/data/archiveit/ArchiveIt-Collection-2950\"\n",
    "val cdxPath = path + \"/cdx/*.cdx.gz\"\n",
    "val warcPath = path + \"/warc\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "val records = ArchiveSpark.load(WarcCdxHdfsSpec(cdxPath, warcPath))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filtering irrelevant records\n",
    "\n",
    "As for link extraction we are not interested in videos, images, stylesheets and any other files except for webpages ([mime type](https://en.wikipedia.org/wiki/Media_type) *text/html*), and we are not interested in webpages that were unavailable when they were crawled either ([status code](https://en.wikipedia.org/wiki/List_of_HTTP_status_codes) == 200), we will filter those out.\n",
    "\n",
    "*It is important to note that this filtering is done only based on metadata, so up to this point ArchiveSpark does not even touch the actual Web archive records, which is the core efficiency feature of ArchiveSpark.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "val pages = records.filter(r => r.mime == \"text/html\" && r.status == 200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By looking at the first record in our remaining dataset, we can see that this indeed is of type *text/html* and was *online* (status 200) at the time of crawl:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{\n",
       "  \"record\" : {\n",
       "    \"surtUrl\" : \"cn,cntv,english)/program/newsupdate/20110504/109544.shtml\",\n",
       "    \"timestamp\" : \"20111222043804\",\n",
       "    \"originalUrl\" : \"http://english.cntv.cn/program/newsupdate/20110504/109544.shtml\",\n",
       "    \"mime\" : \"text/html\",\n",
       "    \"status\" : 200,\n",
       "    \"digest\" : \"YVOEIYJ45I7QNNFBQTCPKIQAQJIE4B46\",\n",
       "    \"redirectUrl\" : \"-\",\n",
       "    \"meta\" : \"-\",\n",
       "    \"compressedSize\" : 10855\n",
       "  }\n",
       "}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pages.peekJson"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Enriching metadata\n",
    "\n",
    "This is the point when ArchiveSpark actually accesses the full records in order to enrich our metadata records with the desired information. To do so, we define the required [Enrich Functions](https://github.com/helgeho/ArchiveSpark/blob/master/docs/EnrichFuncs.md) (`Links`, `LinkUrls`, `LinkTexts`) based on existing ones (`Html`, `SURT`, `HtmlAttribute`, `HtmlText`).\n",
    "\n",
    "`Html.all` extracts all hyperlinks / anchors (tag `a`) from the pages. This results in a list of multiple values, one for each link. From these we want to extract the link target (attribute `href`) of each link. This can be done by changing the dependency of the `Html` Enrich Function using the `ofEach` operation ([see the docs for more details](https://github.com/helgeho/ArchiveSpark/blob/master/docs/Operations.md)) Although this again will result in multiple values, it is only one for each link, so we use the single dependency operation `of` to apply `SURT` on these and convert the URLs into SURT format. Similarly, we apply `HtmlText` on each link to get the anchor text of the link (the default depency of `HtmlText` would be the text of the whole page)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "val Links = Html.all(\"a\")\n",
    "val LinkUrls = SURT.of(HtmlAttribute(\"href\").ofEach(Links))\n",
    "val LinkTexts = HtmlText.ofEach(Links)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To enrich the filtered records in our dataset with the link information, we call `enrich` on it with every Enrich Function that we explicitely want to have in the dataset. As we are not interested in the raw `a` tags, we do not enrich it with `Links` here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "val pagesWithLinks = pages.enrich(LinkUrls).enrich(LinkTexts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A look at the first record shows what we get:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{\n",
       "  \"record\" : {\n",
       "    \"surtUrl\" : \"cn,cntv,english)/program/newsupdate/20110504/109544.shtml\",\n",
       "    \"timestamp\" : \"20111222043804\",\n",
       "    \"originalUrl\" : \"http://english.cntv.cn/program/newsupdate/20110504/109544.shtml\",\n",
       "    \"mime\" : \"text/html\",\n",
       "    \"status\" : 200,\n",
       "    \"digest\" : \"YVOEIYJ45I7QNNFBQTCPKIQAQJIE4B46\",\n",
       "    \"redirectUrl\" : \"-\",\n",
       "    \"meta\" : \"-\",\n",
       "    \"compressedSize\" : 10855\n",
       "  },\n",
       "  \"payload\" : {\n",
       "    \"string\" : {\n",
       "      \"html\" : {\n",
       "        \"a\" : [ {\n",
       "          \"attributes\" : {\n",
       "            \"href\" : {\n",
       "              \"SURT\" : \"cn,cntv,english)/\"\n",
       "            }\n",
       "          },\n",
       "          \"text\" : \"Homepage\"\n",
       "        }, {\n",
       "          \"attributes\" : {\n",
       "            \"href\" : {\n",
       "              \"SURT\" : \"cn,cntv,english)/live\"\n",
       "            }\n",
       "          },\n",
       "          \"text\" : \"CCTV Live\"\n",
       "     ..."
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pagesWithLinks.peekJson"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saving the derived corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we want to save our derived corpus with the link information in this JSON format as shown above, we could simply call `.saveAsJson` on it now. This preserves the entire lineage as it implicitely documents for each value what its parent was and were it was extracted from, with the exact metadata of each record included as well. JSON is a very universal format and can be read by many third-party tools to post-process this datset.\n",
    "\n",
    "*By adding a .gz extension to the output path, the data will be automatically compressed with GZip.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pagesWithLinks.saveAsJson(\"pages-with-links.gz\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving plain links (src, timestamp, dst, text)\n",
    "\n",
    "Instead, we can also keep only the hyperlink information as a temporal edgelist with the source URL, the timestamp of the capture, the destination URL of each link as well as the anchor text if available.\n",
    "\n",
    "There are two preferred ways to achieve this with ArchiveSpark:\n",
    "\n",
    "1. We create a single value using the Enrich Function `Values` that combines destination URL and text for each link. Then, in a map, can access these values and create our very own output format by adding additional information, like source and timestmap.\n",
    "2. We create a single value using the Enrich Function `Values` for each link like before, but this time we include the source and timestamp in this value, so that we only need to [flat map the values (`flatMapValues`)](https://github.com/helgeho/ArchiveSpark/blob/master/docs/Operations.md)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Custom `map`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "val LinkRepresentation = Values(\"link-dst-text\", LinkUrls, LinkTexts).onEach(Links)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{\n",
       "  \"record\" : {\n",
       "    \"surtUrl\" : \"cn,cntv,english)/program/newsupdate/20110504/109544.shtml\",\n",
       "    \"timestamp\" : \"20111222043804\",\n",
       "    \"originalUrl\" : \"http://english.cntv.cn/program/newsupdate/20110504/109544.shtml\",\n",
       "    \"mime\" : \"text/html\",\n",
       "    \"status\" : 200,\n",
       "    \"digest\" : \"YVOEIYJ45I7QNNFBQTCPKIQAQJIE4B46\",\n",
       "    \"redirectUrl\" : \"-\",\n",
       "    \"meta\" : \"-\",\n",
       "    \"compressedSize\" : 10855\n",
       "  },\n",
       "  \"payload\" : {\n",
       "    \"string\" : {\n",
       "      \"html\" : {\n",
       "        \"a\" : [ {\n",
       "          \"attributes\" : {\n",
       "            \"href\" : {\n",
       "              \"SURT\" : \"cn,cntv,english)/\"\n",
       "            }\n",
       "          },\n",
       "          \"text\" : \"Homepage\",\n",
       "          \"link-dst-text\" : [ \"cn,cntv,english)/\", \"Homepage\" ]\n",
       "        }, {\n",
       "          \"attributes\" : {\n",
       "            \"href\" : {\n",
       "              \"SURT\" : \"cn,cntv,english)/live..."
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pagesWithLinks.enrich(LinkRepresentation).peekJson"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "val links = pagesWithLinks.enrich(LinkRepresentation).flatMap { record =>\n",
    "    record.valueOrElse(LinkRepresentation, Seq.empty).map { case Array(dst, text) =>\n",
    "        Seq(record.surtUrl, record.timestamp, dst, text).mkString(\"\\t\")\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print the first 10 lines of this dataset to see what we get:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cn,cntv,english)/program/newsupdate/20110504/109544.shtml\t20111222043804\tcn,cntv,english)/\tHomepage\n",
      "cn,cntv,english)/program/newsupdate/20110504/109544.shtml\t20111222043804\tcn,cntv,english)/live\tCCTV Live\n",
      "cn,cntv,english)/program/newsupdate/20110504/109544.shtml\t20111222043804\tcn,cntv,english)/newsletter\tNewsletter\n",
      "cn,cntv,english)/program/newsupdate/20110504/109544.shtml\t20111222043804\tcn,cntv,english)/english/special/application/contact/index.shtml\tFeedback\n",
      "cn,cntv,english)/program/newsupdate/20110504/109544.shtml\t20111222043804\tcn,cntv,english)/sitemap\tSite Map\n",
      "cn,cntv,english)/program/newsupdate/20110504/109544.shtml\t20111222043804\tcn,cntv,passport)/app_pass/verify/english/new/register.jsp\tSign Up\n",
      "cn,cntv,english)/program/newsupdate/20110504/109544.shtml\t20111222043804\tcn,cntv,passport)/app_pass/verify/english/new/login.jsp\tSign In\n",
      "cn,cntv,english)/program/newsupdate/20110504/109544.shtml\t20111222043804\tcn,cntv,english)/about\tHelp\n",
      "cn,cntv,english)/program/newsupdate/20110504/109544.shtml\t20111222043804\tcn,cntv,english)/01/index.shtml\t\n",
      "cn,cntv,english)/program/newsupdate/20110504/109544.shtml\t20111222043804\tcn,cntv,english)/weather\tWeather\n"
     ]
    }
   ],
   "source": [
    "links.take(10).foreach(println)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save as text file (GZip compressed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "links.saveAsTextFile(\"links.gz\", classOf[GzipCodec])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. `flatMapValues` ([ArchiveSpark Operations](https://github.com/helgeho/ArchiveSpark/blob/master/docs/Operations.md))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A pointer to values in the CDX meta record can be based on the [`Root` Enrich Function](https://github.com/helgeho/ArchiveSpark/blob/master/docs/EnrichFuncs.md):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "val SurtURL = Root[CdxRecord].map(\"surtUrl\") { cdx: CdxRecord => cdx.surtUrl}\n",
    "val Timestamp = Root[CdxRecord].map(\"timestamp\") { cdx: CdxRecord => cdx.timestamp}\n",
    "val LinkRepresentation = Values(\"src-timestamp-dst-text\", SurtURL, Timestamp, LinkUrls, LinkTexts).onEach(Links)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{\n",
       "  \"record\" : {\n",
       "    \"surtUrl\" : \"cn,cntv,english)/program/newsupdate/20110504/109544.shtml\",\n",
       "    \"timestamp\" : \"20111222043804\",\n",
       "    \"originalUrl\" : \"http://english.cntv.cn/program/newsupdate/20110504/109544.shtml\",\n",
       "    \"mime\" : \"text/html\",\n",
       "    \"status\" : 200,\n",
       "    \"digest\" : \"YVOEIYJ45I7QNNFBQTCPKIQAQJIE4B46\",\n",
       "    \"redirectUrl\" : \"-\",\n",
       "    \"meta\" : \"-\",\n",
       "    \"compressedSize\" : 10855\n",
       "  },\n",
       "  \"timestamp\" : \"20111222043804\",\n",
       "  \"payload\" : {\n",
       "    \"string\" : {\n",
       "      \"html\" : {\n",
       "        \"a\" : [ {\n",
       "          \"attributes\" : {\n",
       "            \"href\" : {\n",
       "              \"SURT\" : \"cn,cntv,english)/\"\n",
       "            }\n",
       "          },\n",
       "          \"text\" : \"Homepage\",\n",
       "          \"src-timestamp-dst-text\" : [ \"cn,cntv,english)/program/newsupdate/20110504/109544.shtml\", \"20111222043804\", \"cn,cntv,english)/\",..."
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pagesWithLinks.enrich(SurtURL).enrich(Timestamp).enrich(LinkRepresentation).peekJson"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We concatenate the link properties delimited by a tab (`\\t`) values before saving them as text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "val links = pagesWithLinks.enrich(SurtURL).enrich(Timestamp).flatMapValues(LinkRepresentation).map(_.mkString(\"\\t\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cn,cntv,english)/program/newsupdate/20110504/109544.shtml\t20111222043804\tcn,cntv,english)/\tHomepage\n",
      "cn,cntv,english)/program/newsupdate/20110504/109544.shtml\t20111222043804\tcn,cntv,english)/live\tCCTV Live\n",
      "cn,cntv,english)/program/newsupdate/20110504/109544.shtml\t20111222043804\tcn,cntv,english)/newsletter\tNewsletter\n",
      "cn,cntv,english)/program/newsupdate/20110504/109544.shtml\t20111222043804\tcn,cntv,english)/english/special/application/contact/index.shtml\tFeedback\n",
      "cn,cntv,english)/program/newsupdate/20110504/109544.shtml\t20111222043804\tcn,cntv,english)/sitemap\tSite Map\n",
      "cn,cntv,english)/program/newsupdate/20110504/109544.shtml\t20111222043804\tcn,cntv,passport)/app_pass/verify/english/new/register.jsp\tSign Up\n",
      "cn,cntv,english)/program/newsupdate/20110504/109544.shtml\t20111222043804\tcn,cntv,passport)/app_pass/verify/english/new/login.jsp\tSign In\n",
      "cn,cntv,english)/program/newsupdate/20110504/109544.shtml\t20111222043804\tcn,cntv,english)/about\tHelp\n",
      "cn,cntv,english)/program/newsupdate/20110504/109544.shtml\t20111222043804\tcn,cntv,english)/01/index.shtml\t\n",
      "cn,cntv,english)/program/newsupdate/20110504/109544.shtml\t20111222043804\tcn,cntv,english)/weather\tWeather\n"
     ]
    }
   ],
   "source": [
    "links.take(10).foreach(println)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "links.saveAsTextFile(\"links1.gz\", classOf[GzipCodec])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Graph Analysis\n",
    "\n",
    "To reduce the size of this dataset for this example, we keep only the link graph under *.de*, i.e., only links that point from a *.de* page to a *.de* page are considered:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "val srcDst = pages.filter(_.surtUrl.startsWith(\"de,\")).enrich(LinkUrls).flatMap(r => r.valueOrElse(LinkUrls, Seq.empty).map(dst => (r.surtUrl, dst)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order analyze an extracted Web graph with tools like [Spark's GraphX](https://spark.apache.org/graphx/), we are only interested in the URLs, which need to be assigned IDs first:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "val urlIdMap = srcDst.flatMap{case (src, dst) => Iterator(src, dst)}.distinct.zipWithUniqueId.collectAsMap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "val ids = sc.broadcast(urlIdMap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "val edges = srcDst.map{case (src, dst) => (ids.value(src), ids.value(dst))}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import org.apache.spark.graphx._\n",
    "val graph = Graph.fromEdgeTuples(edges, true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "257067"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph.numVertices"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ArchiveSpark latest (Spark 2.2.0) dynamic alloc",
   "language": "",
   "name": "archivespark_dynamic"
  },
  "language_info": {
   "name": "scala",
   "version": "2.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
