{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading WARC / Generating CDX (enable more efficient processing)\n",
    "\n",
    "ArchiveSpark gains its efficiency through a two-step loading approach, which only accesses metadata for common operations like filtering, sorting, grouping, etc. Only if content is required for applying additional filters or derive new information from a record, ArchiveSpark will access the actual records. The required metadata for Web archives is commonly provided by CDX records. In the following we show how to generate these CDX records from a collection of WARC.gz files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import org.archive.archivespark._\n",
    "import org.archive.archivespark.implicits._\n",
    "import org.archive.archivespark.enrich.functions._\n",
    "import org.archive.archivespark.specific.warc._\n",
    "import org.archive.archivespark.specific.warc.enrichfunctions._\n",
    "import org.archive.archivespark.specific.warc.implicits._\n",
    "import org.archive.archivespark.specific.warc.specs._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the dataset from WARC.gz files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "val path = \"/data/archiveit/ArchiveIt-Collection-2950\"\n",
    "val warcPath = path + \"/warc\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ArchiveSpark provides two [DataSpecs](https://github.com/helgeho/ArchiveSpark/blob/master/docs/DataSpecs.md) to load Web archive records from plain (W)ARC records (without CDX). If the records are stored as \\*.warc.gz files with each record being compressed separately, `WarcGzHdfsSpec` should be used for efficiency reasons, as it allows for splitting these files. Otherwise, `WarcHdfsSpec` enables to load any \\*.arc(.gz) and \\*.warc(.gz) files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "val warc = ArchiveSpark.load(WarcHdfsSpec(warcPath + \"/*.*arc*\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While with `WarcGzHdfsSpec` CDX files with positional information can directly be generated `.saveAsCdx`, with `WarcHdfsSpec` the whole dataset has to be saved along with CDX records `.saveAsWarc(..., generateCdx = true)`, therefore `WarcGzHdfsSpec` is highly recommended if it suits your dataset (see below):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "val warc = ArchiveSpark.load(WarcGzHdfsSpec(warcPath + \"/*.warc.gz\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Take a look at the first record\n",
    "\n",
    "As we can see, although loaded directly from WARC, the records are internally represented in the same format as datasets with provided CDX data. Hence, we can apply the same operations as well Enrich Functions, however, the processing will be less efficient than with available CDX records."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{\n",
       "  \"record\" : {\n",
       "    \"surtUrl\" : \"-\",\n",
       "    \"timestamp\" : \"20111220013002\",\n",
       "    \"originalUrl\" : null,\n",
       "    \"mime\" : \"-\",\n",
       "    \"status\" : 0,\n",
       "    \"digest\" : \"LEPWK3MY3EA6X25EUWXJ452252SZRRXN\",\n",
       "    \"redirectUrl\" : \"-\",\n",
       "    \"meta\" : \"-\",\n",
       "    \"compressedSize\" : 647\n",
       "  }\n",
       "}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "warc.peekJson"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Counting the records in this dataset takes long as all headers and contents are read and parsed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "78993499"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "warc.count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*(took 31 minutes)*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate CDX"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now generate and save the CDX records corresponding to our dataset for a more efficient use of this dataset with ArchiveSpark next time: (by adding .gz to the path, the output will automatically be compressed using GZip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "warc.saveAsCdx(path + \"_cdx.gz\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Re-load dataset with CDX records"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we have CDX records for our dataset now, we can use the `WarcCdxHdfsSpec` to load it more efficiently:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "val records = ArchiveSpark.load(WarcCdxHdfsSpec(path + \"_cdx.gz\", warcPath))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Counting as well as most of the other [operations](https://github.com/helgeho/ArchiveSpark/blob/master/docs/Operations.md) provided by [Spark](https://spark.apache.org/docs/latest/rdd-programming-guide.html) as well as [ArchiveSpark](https://github.com/helgeho/ArchiveSpark/blob/master/docs/Operations.md) will be more efficient now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "78993499"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "records.count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*(took 26 seconds)*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ArchiveSpark latest (Spark 2.2.0) dynamic alloc",
   "language": "",
   "name": "archivespark_dynamic"
  },
  "language_info": {
   "name": "scala",
   "version": "2.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
